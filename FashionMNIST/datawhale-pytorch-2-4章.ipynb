{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25617f81",
   "metadata": {},
   "source": [
    "## 张量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a530046",
   "metadata": {},
   "source": [
    "### 张量是一个数据容器，在pytorch中是我们用到的各式各样的数据，并提供了GPU计算和自动求导等多种功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b553408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.7251, 1.8868, 1.8700],\n",
      "        [1.5331, 1.5078, 1.3017],\n",
      "        [1.7214, 1.4276, 1.6914],\n",
      "        [1.7207, 1.7257, 1.6173]])\n",
      "tensor([[1.7251, 1.8868, 1.8700],\n",
      "        [1.5331, 1.5078, 1.3017],\n",
      "        [1.7214, 1.4276, 1.6914],\n",
      "        [1.7207, 1.7257, 1.6173]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\15620\\anaconda3\\envs\\paiflow\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3281.)\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 不改变原值 和 改变原值函数 + _\n",
    "y = torch.rand(4, 3)\n",
    "x = torch.tensor([1, 1, 1]).T\n",
    "print(torch.add(x, y))\n",
    "y.add_(x) # y的值改变为 x+y的值\n",
    "print(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1dcbcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.add_(1): tensor([[2., 2., 2., 2., 2., 2.]])\n",
      "x: tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# 张量的维度变换方法 ，view处理后的数据和原数据共享内存，只是改变了展现方式；\n",
    "# reshape函数不能确定返回值是否为其拷贝值，推荐用clone函数和view函数替代\n",
    "x = torch.ones(2, 3)\n",
    "y = x.view(1, 6)\n",
    "print(\"y.add_(1):\", y.add_(1))\n",
    "print(\"x:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04251d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.add_(1): tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "x: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# x的值发生了改变，采用clone函数处理，不改变x的值\n",
    "x = torch.ones(2, 3)\n",
    "y = torch.clone(x)\n",
    "print(\"y.add_(1):\", y.add_(1))\n",
    "print(\"x:\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6154400",
   "metadata": {},
   "source": [
    "### 广播机制为常用的矩阵运算机制，遵循以下原则：\n",
    "如果遵守以下规则，则两个tensor是“可广播的”：<br>\n",
    "  每个tensor至少有一个维度；<br>\n",
    "  遍历tensor所有维度时，从末尾随开始遍历，两个tensor存在下列情况：<br>\n",
    "    tensor维度相等。<br>\n",
    "    tensor维度不等且其中一个维度为1。<br>\n",
    "    tensor维度不等且其中一个维度不存在。<br>\n",
    "如果两个tensor是“可广播的”，则计算过程遵循下列规则：<br>\n",
    "    如果两个tensor的维度不同，则在维度较小的tensor的前面增加维度，使它们维度相等。<br>\n",
    "    对于每个维度，计算结果的维度值取两个tensor中较大的那个值。<br>\n",
    "    两个tensor扩展维度的过程是将数值进行复制。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f769c829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2, 3)\n",
    "y = torch.ones(2)\n",
    "(x + y).shape #维度不匹配，y = torch.ones(3)则维度匹配了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62b479ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 3])\n",
      "torch.Size([5, 2, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37364\\4023541364.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 后部分有一个维度大元素个数为1， 可广播\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 后部分y维度对应元素个数与x不对应且维度对应的元素个数不为1，不能广播，\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 2, 3)\n",
    "y = torch.ones(2, 3)\n",
    "#维度不同则在较小维度前加维度，小维度要与较大维度后部分对应，要么值相同要么为1，可广播\n",
    "print((x + y).shape) # 后部分维度对应\n",
    "y = torch.ones(2, 1)\n",
    "print((x + y).shape) # 后部分有一个维度大元素个数为1， 可广播\n",
    "y = torch.ones(2, 2)\n",
    "print((x + y).shape) # 后部分y维度对应元素个数与x不对应且维度对应的元素个数不为1，不能广播，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7852b27a",
   "metadata": {},
   "source": [
    "### 总结，如果要能广播，多维张量相加则需要已有的部分维度对应的元素个数相等或者有不等的维度中存在维度个数为1的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821b82a0",
   "metadata": {},
   "source": [
    "## 自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad2317f",
   "metadata": {},
   "source": [
    "### 具体属性\n",
    "通过对张量的属性。requires_grad设置为True，课计算张量操作后的所有梯度，\n",
    "并且会自动累加到.grad属性;完成计算后，可以通过调用.backward()来自动计算梯度<br>\n",
    "使用.detach()方法将张量与其计算历史隔离，防止此张量被跟踪，可以将代码块包装在 with torch.no_grad(): 中<br>\n",
    "每个张量都有一个.grad_fn属性，该属性引用了创建 Tensor 自身的Function(除非这个张量是用户手动创建的，即这个张量的grad_fn是 None )。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6e4e2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True) # 默认requires_grad为false,无法求导\n",
    "y = x ** 2 # 注意y是x运算后的中间结果，不是此运算的源输入，若想求梯度，需要用retarin_grad方法设置\n",
    "out = (y + x).sum() # 如果backward()不指定参数，梯度运算的输出只能为结果标量才能反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f82f40b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "80304025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7b18336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\15620\\anaconda3\\envs\\paiflow\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6d59dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True) # 默认requires_grad为false,无法求导\n",
    "y = x ** 2 # 想求中间运算结果y梯度，需要用retarin_grad方法设置\n",
    "y.retain_grad()\n",
    "out = (y + x).sum() # 梯度运算的出书只能为结果标量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d6afc",
   "metadata": {},
   "source": [
    "## print(y.grad, y.requires_grad) # 此时中间结果y显示没有梯度，对于实际运算中我们用不到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "948c0be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True) # 默认requires_grad为false,无法求导\n",
    "y = torch.eye(2, requires_grad=True)\n",
    "out1 = (x**2 + y**2).sum() # 梯度运算的出书只能为结果标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "16c1c944",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d992db8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 2.],\n",
       "         [2., 2.]]),\n",
       " tensor([[2., 0.],\n",
       "         [0., 2.]]))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad, y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0947af9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 2.]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "34843fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再来一次反向传播\n",
    "out1 = (x**2 + y**2).sum() \n",
    "out1.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "820a44f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4., 4.],\n",
       "         [4., 4.]]),\n",
       " tensor([[4., 0.],\n",
       "         [0., 4.]]))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad, y.grad #梯度进行了叠加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "625219f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每一次反向传播之前需要将梯度清零\n",
    "out1 = (x**2 + y**2).sum() \n",
    "x.grad.data.zero_() # 将x梯度清零\n",
    "out1.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "53476703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 2.],\n",
       "         [2., 2.]]),\n",
       " tensor([[6., 0.],\n",
       "         [0., 6.]]))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad, y.grad #y没有将梯度数值清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "09e21f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True False\n"
     ]
    }
   ],
   "source": [
    "# 设置不计算梯度代码块\n",
    "print(x.requires_grad)\n",
    "with torch.no_grad():\n",
    "    print(x.requires_grad, (x**2).requires_grad) #x梯度属性不变，计算后的结果不计算梯度属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2e4404fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1,requires_grad=True)\n",
    "\n",
    "print(x.data) # 还是一个tensor\n",
    "print(x.data.requires_grad) # 但是已经是独立于计算图之外\n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值 \n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "15d46a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[201., 201.],\n",
       "        [201., 201.]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "out = (x ** 2 + x).sum()\n",
    "\n",
    "x.data *= 100 # 会影响梯度,因为梯度运算过程中需要用到x, 对x求导梯度为 2 * x + 1\n",
    "out.backward()\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "33b67b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[402., 402.],\n",
       "        [402., 402.]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x ** 2 + x\n",
    "out = y\n",
    "\n",
    "x.data *= 100 # 会影响梯度,因为梯度运算过程中需要用到x, 对x求导梯度为 2 * x + 1\n",
    "out.backward(y) # 输入y后，y为元素为2的2*2矩阵，d(out) / d(x) 的结果需乘y，变为2*（2*x+1）\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d2fc86",
   "metadata": {},
   "source": [
    "## 并行计算简介\n",
    "并行计算应用场景：数据量较大无法在单块GPU上完成/需要提升计算速度.<br>\n",
    "cuda:是英伟达GPU的编程语言；pytorch使用CUDA表示要求我们的模型使用GPU<br>\n",
    "并行方法：<br>1、将模型的不同部分拆分到不同GPU中；<br>2、将同一层的任务分布到不同数据中，让不同GPU训练同一层模型的不同任务 <br>3、将不同数据分布到不同GPU上<br>\n",
    "**注意处理后的注意数据、模型合并**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20148d93",
   "metadata": {},
   "source": [
    "# Pytorch的主要组成模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d9093",
   "metadata": {},
   "source": [
    "## 基本配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "843b75d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "84f6f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # 常用参数，批处理大小，学习率， 最大迭代次数\n",
    "lr = 1e-4\n",
    "max_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1bd8be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' #使用GPU设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e635a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\") #使用GPU设置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c2af5",
   "metadata": {},
   "source": [
    "## 数据读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c4cc2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset定义好数据的格式和数据变换形式，DataLoader用iterative的方式不断读入批次数据\n",
    "# 自定义Dataset类实现灵活的数据读取，需要继承pytorch自身的Dataset类，包含三个函数：\n",
    "# 1.__init__: 用于向类中传入外部参数，同时定义样本集\n",
    "# 2.__getitem__: 用于逐个读取样本集合中的元素，可以进行一定的变换，并将返回训练/验证所需的数据\n",
    "# 3.__len__: 用于返回数据集的样本数 \n",
    "\n",
    "# 迭代式的读取数据：images, labels = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b1fc83",
   "metadata": {},
   "source": [
    "## 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "da3bdcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 类是torch.nn模块里的一个模型构造类，所有神经网络模块的基类；\n",
    "# 无需定义反向传播函数，系统将通过自动求梯度而自动生成反向传播所需的backward函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a744409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(256, 10)\n",
    "    def forward(self, X):\n",
    "        o = self.act(self.hidden(X))\n",
    "        return self.output(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2bf5fe99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0649, -0.0121, -0.0667, -0.0549, -0.0291, -0.0750, -0.0048,  0.0875,\n",
       "         -0.0177, -0.0547],\n",
       "        [-0.2272, -0.1197, -0.1488, -0.0818,  0.0257, -0.2651,  0.0819,  0.0671,\n",
       "          0.0825, -0.1103]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2, 784)\n",
    "net = MLP()\n",
    "print(net)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4a1be4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷积运算Conv2d:矩阵与卷积串口做对位运算再加和，可以调整步长、卷积窗口大小，填充\n",
    "# 池化运算：对输入数据的一个固定形状窗口中的元素计算输出（最大值或者平均值）\n",
    "# LeNet网络：一个简单地前馈神经网络(包含了卷积层，池化层，全连接层，relu层)\n",
    "# AlexNet:用到了dropout\n",
    "# torch.Tensor:一个多维数组，支持自动求导等操作，同时也保存了张量的梯度\n",
    "# nn.Module:神经网络模块，方便封装参数，具有将参数移动到GPU、导出、加载等功能\n",
    "# nn.Parameter: 张量的一种，当它作为属性分配给一个Modeul时，自动注册为一个参数\n",
    "# autograd.Function: 实现了自动求导向前和反向传播的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44490d48",
   "metadata": {},
   "source": [
    "## 模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "87300ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TORCH.NN.INIT:该模块中的所有函数都旨在用于初始化神经网络参数，因此它们都以 torch.no_grad() \n",
    "# 模式运行，不会被 autograd 考虑在内。\n",
    "# 函数的后缀都带有下划线，意味着这些函数将会直接原地更改输入张量的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5786c66f",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c7865236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmoothL1Loss()"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 好的训练离不开优质的负反馈\n",
    "# 二分类交叉熵损失函数\n",
    "torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "# 交叉熵损失函数\n",
    "torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
    "# L1损失函数\n",
    "torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')\n",
    "# MSE损失函数\n",
    "torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "# 平滑L1（Smooth L1）损失函数\n",
    "torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean', beta=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509535c8",
   "metadata": {},
   "source": [
    "## 训练和评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5bf222de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置训练状态：\n",
    "    # 准备数据\n",
    "    # 优化器梯度置为0\n",
    "    # 将数据放入模型中训练\n",
    "    # 计算损失函数\n",
    "    # 对损失函数进行反向传播\n",
    "    # 使用优化器更新参数\n",
    "# 设置测试状态\n",
    "    # 准备数据\n",
    "    # 用已训练模型处理数据\n",
    "    # 确定输出结果\n",
    "    # 计算损失，评估效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f64de46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, label in train_loader:\n",
    "        data, label = data.cuda(), label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(label, output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    print('epoch:{}\\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a164c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    model.val()\n",
    "    val_loss = 0\n",
    "    for data, label in data.cuda(), label.cuda():\n",
    "        output = model(data)\n",
    "        preds = torch.argmax(output, 1)\n",
    "        loss = criterion(label, output)\n",
    "        val_loss += loss.item()*data.size(0)\n",
    "        running_accu += torch.sum(preds == label.data)\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    print('epoch: {}\\tTraining Loss: {:.6f}'.forrmat(epoch, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c51b48f",
   "metadata": {},
   "source": [
    "## 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f883f7a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (3424457513.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\15620\\AppData\\Local\\Temp\\ipykernel_37364\\3424457513.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    pytorch中所有优化器的基类为Optimizer,定义为：\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "# 优化器根据网络反向传播的梯度信息更新网络的参数，起到降低loss函数计算值，使得模型输出更加接近真实标签\n",
    "###\n",
    "pytorch中所有优化器的基类为Optimizer,定义为：\n",
    "class Optimizer(object):\n",
    "    def __init__(self, params, defaults):\n",
    "        self.defaults = defaults #存储的优化器的超参数\n",
    "        self.state = defaultdict(dict) # 存储的参数的缓存\n",
    "        self.params_groups = [] # 管理的参数组，是一个list,每个元素是一个字典\n",
    "        \n",
    "Optimizer的方法有：\n",
    "zero_grad()：清空所管理的梯度\n",
    "step(): 行一步梯度更新，参数更新\n",
    "add_param_group():填加参数组，添加的元素是字典,字典对应的值是张量\n",
    "load_state_dict():加载状态参数字典，可以用来进行模型的断点训练，继续上次的参数进行训练\n",
    "state_dict():获取优化器当前状态信息字典\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "50ef532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data of weight before step:\n",
      "tensor([[-0.0320, -1.9676],\n",
      "        [ 2.0797,  1.9269]])\n",
      "The grad of weight before step:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "The data of weight after step:\n",
      "tensor([[-0.1320, -2.0676],\n",
      "        [ 1.9797,  1.8269]])\n",
      "The grad of weight after step:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "The grad of weight after optimizer.zero_grad():\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "optimizer.params_group is \n",
      "[{'params': [tensor([[-0.1320, -2.0676],\n",
      "        [ 1.9797,  1.8269]], requires_grad=True)], 'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False}]\n",
      "weight in optimizer:3142127689464\n",
      "weight in weight:3142127689464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weight = torch.randn((2, 2), requires_grad=True)\n",
    "weight.grad = torch.ones((2, 2))\n",
    "print(\"The data of weight before step:\\n{}\".format(weight.data))\n",
    "print(\"The grad of weight before step:\\n{}\".format(weight.grad))\n",
    "# 实例化优化器\n",
    "optimizer = torch.optim.SGD([weight], lr=0.1, momentum=0.9)\n",
    "optimizer.step()\n",
    "print(\"The data of weight after step:\\n{}\".format(weight.data))\n",
    "print(\"The grad of weight after step:\\n{}\".format(weight.grad))\n",
    "optimizer.zero_grad()\n",
    "print(\"The grad of weight after optimizer.zero_grad():\\n{}\".format(weight.grad))\n",
    "# 输出参数\n",
    "print(\"optimizer.params_group is \\n{}\".format(optimizer.param_groups))\n",
    "# 查看参数位置，optimizer和weight的位置一样，我觉得这里可以参考Python是基于值管理\n",
    "print(\"weight in optimizer:{}\\nweight in weight:{}\\n\".format(id(optimizer.param_groups[0]['params'][0]), id(weight)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ca53f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397dc0f1",
   "metadata": {},
   "source": [
    "# 基础实战——FashionMNIST时装分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "6b832ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "246074e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 256\n",
    "lr = 1e-4\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e68484ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "image_size = 28\n",
    "data_transform = transforms.Compose([\n",
    "    #transforms.ToPILImage(),  \n",
    "     # 这一步取决于后续的数据读取方式，如果使用内置数据集读取方式则不需要\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "17076809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "train_data = datasets.FashionMNIST(root='./', train=True, download=True, transform=data_transform)\n",
    "test_data = datasets.FashionMNIST(root='./', train=False, download=True, transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "e73addb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f95c97c",
   "metadata": {},
   "source": [
    "#### 卡柱，问题没有解决,数据格式不对，直接使用内置数据集合读取，如果使用csv文件，在进行ToPILImage文件转换时 出现通道数不对应情况（csv文件中的维度为28维）\n",
    "class FMDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.images = torch.from_numpy(df.iloc[:,1:].values)\n",
    "        self.labels = df.iloc[:, 0].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].reshape(28,28,1)\n",
    "        label = int(self.labels[idx])\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.tensor(image/255., dtype=torch.float)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "train_df = pd.read_csv(\"./FashionMNIST/raw/fashion-mnist_train.csv\")\n",
    "test_df = pd.read_csv(\"./FashionMNIST/raw/fashion-mnist_test.csv\")\n",
    "train_data = FMDataset(train_df, data_transform)\n",
    "test_data = FMDataset(test_df, data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "a3d3ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc99b5ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "53a7e6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 28, 28]) torch.Size([256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2db9c03c248>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAc8ElEQVR4nO3df2xV5R3H8c+l0EvBS7VCe2+l1GpgLJSRKQzo/FGMNjYZmeIS1GSBZDM6fiSkGjNGMhv/oMZFwh9MlunCIJPJP+pMIGIXaJljGMQ6EBnWUaSMdpUivaXUFtpnfxBvdqWAz+Hefnvb9ys5Cffc8+l5ejjNp6f33ueEnHNOAAAYGGU9AADAyEUJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwMxo6wF8U39/v06dOqVIJKJQKGQ9HACAJ+ecOjs7VVhYqFGjrn6tM+RK6NSpUyoqKrIeBgDgOjU3N2vy5MlX3WbIlVAkErEeAoaY559/3jszY8aMQPsKcv4FuWI/efKkd2b37t3emfvvv987I0klJSXembNnz3pnGhoavDO//vWvvTOw8W1+ntJWQi+//LJ+85vfqKWlRTNmzND69et19913XzPHn+DwTWPHjvXOjB8/PtC+guSCnLPjxo3zzmRnZw/KfiTphhtu8M5cuHDBOxPk/xaZ49v8bKTljQnbtm3TqlWrtGbNGjU0NOjuu+9WZWWlTpw4kY7dAQAyVFpKaN26dfrZz36mn//85/rud7+r9evXq6ioSBs3bkzH7gAAGSrlJdTb26sDBw6ooqIiaX1FRYX27t172fY9PT2Kx+NJCwBgZEh5CZ0+fVp9fX0qKChIWl9QUKDW1tbLtq+pqVFubm5i4Z1xADBypO3Dqt98Qco5N+CLVKtXr1ZHR0diaW5uTteQAABDTMrfHTdx4kRlZWVddtXT1tZ22dWRJIXDYYXD4VQPAwCQAVJ+JZSdna0777xTtbW1Setra2tVVlaW6t0BADJYWj4nVFVVpZ/+9KeaPXu25s+fr9///vc6ceKEnnrqqXTsDgCQodJSQosXL1Z7e7uef/55tbS0qLS0VDt27FBxcXE6dgcAyFBpmzFh2bJlWrZsWbq+PEaQSZMmeWeCvs4YZF9B/Pe///XOPPnkk96ZW2+91TsjacB3sl7Lbbfd5p35/PPPvTMYXriVAwDADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADMh55yzHsT/i8fjys3NtR4GhpAgd9s9ePBgoH0dPXrUOxONRr0zN9xwg3cmJyfHOzN6dLA5iv/9738Hyvn6zne+450JMpHrJ5984p3B9evo6NCECROuug1XQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM8Gm2AUCuvXWW70z15qFdyDd3d3eGSnYjNjZ2dmB9uVr37593pkgx06Sxo8f750JMmN3QUGBd6asrMw7wyzaQxdXQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMwwgSkG1Zw5c7wzp06d8s50dHR4ZyQpEol4Z/r6+rwzZ8+e9c4EOXZBJ3L94osvvDOjRvn/TtvU1OSd6e3t9c5g6OJKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBkmMMWgmj59uncmyCSXQScwDSLIpKdBMnfccYd35siRI94ZSWpra/POnD592juTnZ3tnbn55pu9Mxi6uBICAJihhAAAZlJeQtXV1QqFQklLNBpN9W4AAMNAWl4TmjFjhv76178mHmdlZaVjNwCADJeWEho9ejRXPwCAa0rLa0KNjY0qLCxUSUmJHn30UR07duyK2/b09CgejyctAICRIeUlNHfuXG3ZskU7d+7UK6+8otbWVpWVlam9vX3A7WtqapSbm5tYioqKUj0kAMAQlfISqqys1COPPKKZM2fq/vvv1/bt2yVJmzdvHnD71atXq6OjI7E0NzenekgAgCEq7R9WHT9+vGbOnKnGxsYBnw+HwwqHw+keBgBgCEr754R6enp05MgRxWKxdO8KAJBhUl5CzzzzjOrr69XU1KT3339fP/nJTxSPx7VkyZJU7woAkOFS/ue4kydP6rHHHtPp06c1adIkzZs3T/v27VNxcXGqdwUAyHApL6HXX3891V8Sw0iQySeDTKbZ1dXlnZGCfbA6yGfient7vTOfffaZd+amm27yzkhSYWGhd+bjjz/2zvzrX//yzrS0tHhnMHQxdxwAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzab+pHfD/8vPzvTOtra3emQsXLnhnJCk7O9s709nZ6Z2ZMmWKd+b73/++d6ahocE7I0nxeNw7E2RS1iA3tOzo6PDOYOjiSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIZZtBHYmDFjvDPjxo3zznz11VfemaysLO9M0Fx3d7d3Zv78+d6ZM2fOeGecc94ZSQqFQt6Z8+fPe2cikYh35ssvv/TOYOjiSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZJjBFYNFo1DsTZILQ7Oxs70xQQcZXWlrqndm2bZt3Zt68ed6ZnJwc74wUbFLWcDgcaF++WltbB2U/GBxcCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADDDBKYIrKCgwDvT19fnnYnH496ZsWPHemeCmj59undmzpw53plDhw55Z8aMGeOdkaS2tjbvzIULF7wzPT093pkTJ054ZzB0cSUEADBDCQEAzHiX0J49e7Rw4UIVFhYqFArprbfeSnreOafq6moVFhYqJydH5eXlOnz4cKrGCwAYRrxLqKurS7NmzdKGDRsGfP7FF1/UunXrtGHDBu3fv1/RaFQPPPCAOjs7r3uwAIDhxfuNCZWVlaqsrBzwOeec1q9frzVr1mjRokWSpM2bN6ugoEBbt27Vk08+eX2jBQAMKyl9TaipqUmtra2qqKhIrAuHw7r33nu1d+/eATM9PT2Kx+NJCwBgZEhpCX197/dvvnW3oKDgiveFr6mpUW5ubmIpKipK5ZAAAENYWt4dFwqFkh475y5b97XVq1ero6MjsTQ3N6djSACAISilH1aNRqOSLl0RxWKxxPq2trYrfrAxHA4rHA6nchgAgAyR0iuhkpISRaNR1dbWJtb19vaqvr5eZWVlqdwVAGAY8L4SOnfunD777LPE46amJn300UfKy8vTlClTtGrVKq1du1ZTp07V1KlTtXbtWo0bN06PP/54SgcOAMh83iX0wQcfaMGCBYnHVVVVkqQlS5boj3/8o5599ll1d3dr2bJl+vLLLzV37ly9++67ikQiqRs1AGBY8C6h8vJyOeeu+HwoFFJ1dbWqq6uvZ1zIADfddJN3ZtQo/78AB52EM4jvfe973pn3338/DSO53IQJE7wzZ86cScNIUifIBKb9/f1pGAmsMHccAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMBMSu+sipHlSrdsv5rz5897Z4LMmjx6dLBTe9q0ad6ZH/7wh4H25auxsdE7c6U7Gl/L1WbKv5Igd0huaWnxzmB44UoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGSYwRWCjRvn/DhMkE2QyzSATkUrSp59+6p35+OOPA+3L17lz57wzeXl5gfYVZALYIBPa/uc///HOYHjhSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZJjBFYLFYzDvT2dnpnenr6/POTJ8+3TsjSa+++mqg3GCYNGmSd6a7uzsNIxlYVlaWd2awJn/F0MWVEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADNMYIrAbr75Zu9Mb2+vd+bGG28clIwkvf7664Fyg+H8+fPembFjxwbaV3Z2tncmyP9ta2urdwbDC1dCAAAzlBAAwIx3Ce3Zs0cLFy5UYWGhQqGQ3nrrraTnly5dqlAolLTMmzcvVeMFAAwj3iXU1dWlWbNmacOGDVfc5sEHH1RLS0ti2bFjx3UNEgAwPHm/MaGyslKVlZVX3SYcDisajQYeFABgZEjLa0J1dXXKz8/XtGnT9MQTT6itre2K2/b09CgejyctAICRIeUlVFlZqddee027du3SSy+9pP379+u+++5TT0/PgNvX1NQoNzc3sRQVFaV6SACAISrlnxNavHhx4t+lpaWaPXu2iouLtX37di1atOiy7VevXq2qqqrE43g8ThEBwAiR9g+rxmIxFRcXq7GxccDnw+GwwuFwuocBABiC0v45ofb2djU3NysWi6V7VwCADON9JXTu3Dl99tlnicdNTU366KOPlJeXp7y8PFVXV+uRRx5RLBbT8ePH9atf/UoTJ07Uww8/nNKBAwAyn3cJffDBB1qwYEHi8dev5yxZskQbN27UoUOHtGXLFp09e1axWEwLFizQtm3bFIlEUjdqAMCw4F1C5eXlcs5d8fmdO3de14CQOb744gvvzB133OGduemmm7wzFy9e9M5IUktLS6DcYGhvb/fOBP0zeFZWlncmyASrX331lXcGwwtzxwEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzKT9zqoYvtra2rwzkyZN8s7E43HvTJAZnYe622+/3TvT29ubhpEMjDskIwiuhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJhhAlME1tnZ6Z258cYbvTMXLlzwzpw+fdo7I0kTJ04ctH35mjp1qnfmww8/DLSvSCTinTl27FigffkaNcr/d+f+/v40jASpwJUQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM0xgisD++c9/emfGjh3rncnNzfXOhMNh74wkzZ071zuzfft270yQiVKbm5u9M11dXd4ZSRo3bpx35siRI4H25SsrK8s7wwSmQxdXQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMwwgSkC6+zs9M4EmVg0SGbMmDHeGUm67bbbAuV8TZkyxTsT5DhkZ2d7ZyTJOeed+fDDDwPty1dfX9+g7AeDgyshAIAZSggAYMarhGpqajRnzhxFIhHl5+froYce0tGjR5O2cc6purpahYWFysnJUXl5uQ4fPpzSQQMAhgevEqqvr9fy5cu1b98+1dbW6uLFi6qoqEi6cdaLL76odevWacOGDdq/f7+i0ageeOCBQK8fAACGN683JrzzzjtJjzdt2qT8/HwdOHBA99xzj5xzWr9+vdasWaNFixZJkjZv3qyCggJt3bpVTz75ZOpGDgDIeNf1mlBHR4ckKS8vT5LU1NSk1tZWVVRUJLYJh8O69957tXfv3gG/Rk9Pj+LxeNICABgZApeQc05VVVW66667VFpaKklqbW2VJBUUFCRtW1BQkHjum2pqapSbm5tYioqKgg4JAJBhApfQihUrdPDgQf35z3++7LlQKJT02Dl32bqvrV69Wh0dHYmlubk56JAAABkm0IdVV65cqbffflt79uzR5MmTE+uj0aikS1dEsVgssb6tre2yq6OvhcPhQB/CAwBkPq8rIeecVqxYoTfeeEO7du1SSUlJ0vMlJSWKRqOqra1NrOvt7VV9fb3KyspSM2IAwLDhdSW0fPlybd26VX/5y18UiUQSr/Pk5uYqJydHoVBIq1at0tq1azV16lRNnTpVa9eu1bhx4/T444+n5RsAAGQurxLauHGjJKm8vDxp/aZNm7R06VJJ0rPPPqvu7m4tW7ZMX375pebOnat3331XkUgkJQMGAAwfXiX0bSY1DIVCqq6uVnV1ddAxYRg7ceKEdybIZJ///wFqH4WFhYFyvgbrl7KvPz7hq7e31zvT1tYWaF+++vv7B2U/GBzMHQcAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMBPozqpAUHv27PHOrFixwjtz4cIF74wk3XLLLYFyvoLMoj2YdyD+9NNPB21fGNm4EgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGCUwxqIJMjNnT0+OdycrK8s5IUkFBQaCcr2g06p254YYbvDPt7e3eGUl6//33A+V8BZmUNcj5gKGLKyEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmmMAUg+rkyZPemUgk4p05c+aMd0aSamtrA+V8VVRUeGeCHIegE7L29fUFyvnq7+8flP1g6OJKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBkmMMWg+vvf/+6dOXbsmHcmFot5ZyRp+vTpgXK+Jk+e7J3p6enxzjQ3N3tnJOmVV14JlPM1WBOlYujiSggAYIYSAgCY8SqhmpoazZkzR5FIRPn5+XrooYd09OjRpG2WLl2qUCiUtMybNy+lgwYADA9eJVRfX6/ly5dr3759qq2t1cWLF1VRUaGurq6k7R588EG1tLQklh07dqR00ACA4cHrjQnvvPNO0uNNmzYpPz9fBw4c0D333JNYHw6HFY1GUzNCAMCwdV2vCXV0dEiS8vLyktbX1dUpPz9f06ZN0xNPPKG2trYrfo2enh7F4/GkBQAwMgQuIeecqqqqdNddd6m0tDSxvrKyUq+99pp27dqll156Sfv379d99913xbeX1tTUKDc3N7EUFRUFHRIAIMME/pzQihUrdPDgQb333ntJ6xcvXpz4d2lpqWbPnq3i4mJt375dixYtuuzrrF69WlVVVYnH8XicIgKAESJQCa1cuVJvv/229uzZc80P3cViMRUXF6uxsXHA58PhsMLhcJBhAAAynFcJOee0cuVKvfnmm6qrq1NJSck1M+3t7Wpubg78CXYAwPDl9ZrQ8uXL9ac//Ulbt25VJBJRa2urWltb1d3dLUk6d+6cnnnmGf3jH//Q8ePHVVdXp4ULF2rixIl6+OGH0/INAAAyl9eV0MaNGyVJ5eXlSes3bdqkpUuXKisrS4cOHdKWLVt09uxZxWIxLViwQNu2bVMkEknZoAEAw4P3n+OuJicnRzt37ryuAQEARg5m0cagutYvMgM5c+aMd2bGjBneGUkaPXpwfiQ6OzsHZT/9/f2Dsp+ggpwPGF6YwBQAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZJjDFkHf8+HHvTNCJMV999dVAOV/vvPOOdybI7VA++ugj7wwwmLgSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAICZITd3XNA5vzB8dXd3e2e6uroC7evixYuBcr56enq8M0G+pyDHbjDx8z68fZv/35AbYmfByZMnVVRUZD0MAMB1am5u1uTJk6+6zZArof7+fp06dUqRSEShUCjpuXg8rqKiIjU3N2vChAlGI7THcbiE43AJx+ESjsMlQ+E4OOfU2dmpwsJCjRp19Vd9htyf40aNGnXN5pwwYcKIPsm+xnG4hONwCcfhEo7DJdbHITc391ttxxsTAABmKCEAgJmMKqFwOKznnntO4XDYeiimOA6XcBwu4ThcwnG4JNOOw5B7YwIAYOTIqCshAMDwQgkBAMxQQgAAM5QQAMBMRpXQyy+/rJKSEo0dO1Z33nmn/va3v1kPaVBVV1crFAolLdFo1HpYabdnzx4tXLhQhYWFCoVCeuutt5Ked86purpahYWFysnJUXl5uQ4fPmwz2DS61nFYunTpZefHvHnzbAabJjU1NZozZ44ikYjy8/P10EMP6ejRo0nbjITz4dsch0w5HzKmhLZt26ZVq1ZpzZo1amho0N13363KykqdOHHCemiDasaMGWppaUkshw4dsh5S2nV1dWnWrFnasGHDgM+/+OKLWrdunTZs2KD9+/crGo3qgQceUGdn5yCPNL2udRwk6cEHH0w6P3bs2DGII0y/+vp6LV++XPv27VNtba0uXryoioqKpMldR8L58G2Og5Qh54PLED/4wQ/cU089lbRu+vTp7pe//KXRiAbfc88952bNmmU9DFOS3Jtvvpl43N/f76LRqHvhhRcS67766iuXm5vrfve73xmMcHB88zg459ySJUvcj3/8Y5PxWGlra3OSXH19vXNu5J4P3zwOzmXO+ZARV0K9vb06cOCAKioqktZXVFRo7969RqOy0djYqMLCQpWUlOjRRx/VsWPHrIdkqqmpSa2trUnnRjgc1r333jvizg1JqqurU35+vqZNm6YnnnhCbW1t1kNKq46ODklSXl6epJF7PnzzOHwtE86HjCih06dPq6+vTwUFBUnrCwoK1NraajSqwTd37lxt2bJFO3fu1CuvvKLW1laVlZWpvb3demhmvv7/H+nnhiRVVlbqtdde065du/TSSy9p//79uu+++wLduygTOOdUVVWlu+66S6WlpZJG5vkw0HGQMud8GHKzaF/NN2/t4Jy7bN1wVllZmfj3zJkzNX/+fN1+++3avHmzqqqqDEdmb6SfG5K0ePHixL9LS0s1e/ZsFRcXa/v27Vq0aJHhyNJjxYoVOnjwoN57773LnhtJ58OVjkOmnA8ZcSU0ceJEZWVlXfabTFtb22W/8Ywk48eP18yZM9XY2Gg9FDNfvzuQc+NysVhMxcXFw/L8WLlypd5++23t3r076dYvI+18uNJxGMhQPR8yooSys7N15513qra2Nml9bW2tysrKjEZlr6enR0eOHFEsFrMeipmSkhJFo9Gkc6O3t1f19fUj+tyQpPb2djU3Nw+r88M5pxUrVuiNN97Qrl27VFJSkvT8SDkfrnUcBjJkzwfDN0V4ef31192YMWPcH/7wB/fJJ5+4VatWufHjx7vjx49bD23QPP30066urs4dO3bM7du3z/3oRz9ykUhk2B+Dzs5O19DQ4BoaGpwkt27dOtfQ0OA+//xz55xzL7zwgsvNzXVvvPGGO3TokHvsscdcLBZz8XjceOSpdbXj0NnZ6Z5++mm3d+9e19TU5Hbv3u3mz5/vbrnllmF1HH7xi1+43NxcV1dX51paWhLL+fPnE9uMhPPhWschk86HjCkh55z77W9/64qLi112dra74447kt6OOBIsXrzYxWIxN2bMGFdYWOgWLVrkDh8+bD2stNu9e7eTdNmyZMkS59ylt+U+99xzLhqNunA47O655x536NAh20GnwdWOw/nz511FRYWbNGmSGzNmjJsyZYpbsmSJO3HihPWwU2qg71+S27RpU2KbkXA+XOs4ZNL5wK0cAABmMuI1IQDA8EQJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMDM/wBl+plu9porxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = next(iter(train_loader))\n",
    "print(image.shape, label.shape)\n",
    "plt.imshow(image[0][0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "26f0af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(32, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Dropout(0.3))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*4*4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 64*4*4)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "0c3a09d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定损失函数\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "249c3936",
   "metadata": {},
   "outputs": [],
   "source": [
    "?nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "d16f1526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "97f385b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, label in train_loader:\n",
    "        data, label = data.cuda(), label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0) # 求一个epoch的损失和\n",
    "    train_loss = train_loss/len(train_loader.dataset) # 求epoch的平均损失\n",
    "    print(\"Epoch:{}\\tTraining Loss: {:.6f}\".format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "9ebd8161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    gt_labels = []\n",
    "    pred_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "            output = model(data)\n",
    "            preds = torch.argmax(output, 1)\n",
    "            gt_labels.append(label.cpu().data.numpy())\n",
    "            pred_labels.append(preds.cpu().data.numpy())\n",
    "            loss = criterion(output, label)\n",
    "            val_loss += loss.item()*data.size(0)\n",
    "    val_loss = val_loss/len(test_loader.dataset)\n",
    "    gt_labels, pred_labels = np.concatenate(gt_labels), np.concatenate(pred_labels)\n",
    "    acc = np.sum(gt_labels == pred_labels) / len(pred_labels)\n",
    "    print(\"epoch:{}\\tValidation Loss: {:.6f}, Accuracy:{:.6f}\".format(epoch, val_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "94f962b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\tTraining Loss: 0.670686\n",
      "epoch:1\tValidation Loss: 0.455436, Accuracy:0.836600\n",
      "Epoch:2\tTraining Loss: 0.421734\n",
      "epoch:2\tValidation Loss: 0.367584, Accuracy:0.869600\n",
      "Epoch:3\tTraining Loss: 0.362921\n",
      "epoch:3\tValidation Loss: 0.330803, Accuracy:0.880000\n",
      "Epoch:4\tTraining Loss: 0.326666\n",
      "epoch:4\tValidation Loss: 0.320874, Accuracy:0.880900\n",
      "Epoch:5\tTraining Loss: 0.301207\n",
      "epoch:5\tValidation Loss: 0.295217, Accuracy:0.895100\n",
      "Epoch:6\tTraining Loss: 0.285744\n",
      "epoch:6\tValidation Loss: 0.268054, Accuracy:0.902200\n",
      "Epoch:7\tTraining Loss: 0.270659\n",
      "epoch:7\tValidation Loss: 0.269887, Accuracy:0.901500\n",
      "Epoch:8\tTraining Loss: 0.257946\n",
      "epoch:8\tValidation Loss: 0.266417, Accuracy:0.902300\n",
      "Epoch:9\tTraining Loss: 0.247674\n",
      "epoch:9\tValidation Loss: 0.247118, Accuracy:0.909100\n",
      "Epoch:10\tTraining Loss: 0.239210\n",
      "epoch:10\tValidation Loss: 0.257759, Accuracy:0.905900\n",
      "Epoch:11\tTraining Loss: 0.231137\n",
      "epoch:11\tValidation Loss: 0.244539, Accuracy:0.912500\n",
      "Epoch:12\tTraining Loss: 0.221423\n",
      "epoch:12\tValidation Loss: 0.233672, Accuracy:0.916900\n",
      "Epoch:13\tTraining Loss: 0.216682\n",
      "epoch:13\tValidation Loss: 0.236214, Accuracy:0.913400\n",
      "Epoch:14\tTraining Loss: 0.209144\n",
      "epoch:14\tValidation Loss: 0.233478, Accuracy:0.915600\n",
      "Epoch:15\tTraining Loss: 0.201108\n",
      "epoch:15\tValidation Loss: 0.248429, Accuracy:0.907700\n",
      "Epoch:16\tTraining Loss: 0.196019\n",
      "epoch:16\tValidation Loss: 0.232306, Accuracy:0.913800\n",
      "Epoch:17\tTraining Loss: 0.190945\n",
      "epoch:17\tValidation Loss: 0.227086, Accuracy:0.921200\n",
      "Epoch:18\tTraining Loss: 0.184796\n",
      "epoch:18\tValidation Loss: 0.226326, Accuracy:0.922000\n",
      "Epoch:19\tTraining Loss: 0.175758\n",
      "epoch:19\tValidation Loss: 0.227824, Accuracy:0.918900\n",
      "Epoch:20\tTraining Loss: 0.173272\n",
      "epoch:20\tValidation Loss: 0.236903, Accuracy:0.913800\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    val(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "59a8beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型保存\n",
    "save_path = \"./FashionMNIST/FashionModel.pkl\"\n",
    "torch.save(model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16584a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
